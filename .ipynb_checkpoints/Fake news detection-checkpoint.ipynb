{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "digital-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bronze-alias",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20791</th>\n",
       "      <td>Lawyer Who Kept Hillary Campaign Chief Out of ...</td>\n",
       "      <td>Daniel Greenfield</td>\n",
       "      <td>Lawyer Who Kept Hillary Campaign Chief Out of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20792</th>\n",
       "      <td>Jakarta Bombing Kills Three Police Officers, L...</td>\n",
       "      <td>John Hayward</td>\n",
       "      <td>Two suicide bombers attacked a bus station in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16640 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "id                                                         \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1      FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2                      Why the Truth Might Get You Fired   \n",
       "3      15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4      Iranian woman jailed for fictional unpublished...   \n",
       "...                                                  ...   \n",
       "20791  Lawyer Who Kept Hillary Campaign Chief Out of ...   \n",
       "20792  Jakarta Bombing Kills Three Police Officers, L...   \n",
       "20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "\n",
       "                                          author  \\\n",
       "id                                                 \n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "20791                          Daniel Greenfield   \n",
       "20792                               John Hayward   \n",
       "20795                              Jerome Hudson   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "\n",
       "                                                    text  label  \n",
       "id                                                               \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "...                                                  ...    ...  \n",
       "20791  Lawyer Who Kept Hillary Campaign Chief Out of ...      1  \n",
       "20792  Two suicide bombers attacked a bus station in ...      0  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "20797  The Macy’s of today grew from the union of sev...      0  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "\n",
       "[16640 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news=pd.read_csv('C:/Users/Azerty/Desktop/Fake news detection/data/train_news.csv',index_col=0)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "synthetic-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16640 entries, 0 to 20798\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   16194 non-null  object\n",
      " 1   author  15071 non-null  object\n",
      " 2   text    16610 non-null  object\n",
      " 3   label   16640 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 650.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extraordinary-wonder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      446\n",
       "author    1569\n",
       "text        30\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of nan values\n",
    "df_news.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minor-paraguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows with nan values in title and text columns\n",
    "df_news[['title','text']].isna().all(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caring-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.fillna('0',inplace=True)\n",
    "df_news['original_text']=df_news['title']+' '+df_news['text']\n",
    "df_news.drop(columns=['author','text','title'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wooden-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokonization\n",
    "df_news['tokenized_text'] = df_news.apply(lambda x: word_tokenize(x['original_text'].lower()),axis=1)\n",
    "df_news.drop(columns=['original_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pressing-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list=stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process(list_word):\n",
    "    \n",
    "    #delete stop words\n",
    "    filtered_sentence =[word for word in list_word if word not in stopwords_list ]\n",
    "    #delete punctuation\n",
    "    filtered_sentence = [word for word in filtered_sentence  if word.isalnum()]\n",
    "    #lemitization\n",
    "    filtered_sentence = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
    "    \n",
    "    return filtered_sentence\n",
    "\n",
    "df_news['tokenized_text']=df_news.apply(lambda x: process(x['tokenized_text']),axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-petroleum",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "another-cruise",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-da4f19d52806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mfake_text_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_news\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_news\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfake_text_list_concat\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfake_text_list\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdict_fake_text_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrequency_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_text_list_concat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-da4f19d52806>\u001b[0m in \u001b[0;36mfrequency_list\u001b[1;34m(list_word)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0munique_words\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_words\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdict_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfake_text_list_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict_freq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## check the frequent words in each class\n",
    "\n",
    "def frequency_list(list_word):\n",
    "    dict_freq={}\n",
    "    unique_words= np.unique(np.array(list_word))\n",
    "    for word in unique_words :\n",
    "        dict_freq[word]=fake_text_list_concat.count(word)\n",
    "    \n",
    "    return dict_freq\n",
    "\n",
    "\n",
    "## wordcloud : for text (fake)\n",
    "fake_text_list=df_news[df_news['label']==0]['tokenized_text']\n",
    "fake_text_list_concat= [y for x in fake_text_list  for y in x]\n",
    "dict_fake_text_freq=frequency_list(fake_text_list_concat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the world wloud for fake news\n",
    "\n",
    "wordcloud = WordCloud(width=1000,height=1000)\n",
    "wordcloud.generate_from_frequencies(frequencies=dict_fake_text_freq)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wordcloud : for text (real)\n",
    "real_text_list_concat= [y for x in df_news[df_news['label']==1]['tokenized_text'] for y in x]\n",
    "dict_real_text_freq=frequency_list(real_text_list_concat)\n",
    "\n",
    "#generate the world cloud for real news\n",
    "wordcloud.generate_from_frequencies(frequencies=dict_real_text_freq)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-detector",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding using word2vec\n",
    "W2vecmodel = Word2Vec(sentences=df_news['tokenized_text'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Retrieve the weights from the model. This is used for initializing the weights\n",
    "# in a Keras Embedding layer later\n",
    "vocab_size, embedding_size = W2vecmodel.wv.vectors.shape\n",
    "\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "vector = W2vecmodel.wv['trump']  # get numpy vector of a word\n",
    "similars = W2vecmodel.wv.most_similar('trump', topn=10)\n",
    "similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-bloom",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to sequance (for lstm)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df_news['tokenized_text'])\n",
    "df_news['tokenized_text']=tokenizer.texts_to_sequences(df_news['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of word per text\n",
    "sns.histplot([len(word) for word in df_news['tokenized_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr=np.array([len(word) for word in df_news['tokenized_text']])\n",
    "len(Arr[Arr>1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pad_sequences(df_news['tokenized_text'],maxlen=1500) # i choosed as text size 1500 : text with more than 1500 will be trunked and text with less than 1500 will be padded\n",
    "y=df_news['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrixweight(W2vecmodel):\n",
    "    matrix=np.zeros((vocab_size+1,100))\n",
    "    for i, word in tokenizer.index_word.items():\n",
    "        matrix[i]=W2vecmodel.wv[word]\n",
    "    return matrix\n",
    "\n",
    "embedding_matrix=matrixweight(W2vecmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-sewing",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size+1, output_dim=embedding_size,weights=[embedding_matrix],trainable=False,input_length=1500,mask_zero=True))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history=model.fit(X,y, validation_split=0.2,epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss and Accuracy Graphs\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-projector",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('C:/Users/Azerty/Desktop/Fake news detection/data/test_news.csv',index_col=0)\n",
    "#preprocess the test data\n",
    "test_data.fillna('0',inplace=True)\n",
    "test_data['original_text']=test_data['title']+' '+test_data['text']\n",
    "test_data.drop(columns=['author','text','title'],inplace=True)\n",
    "test_data['tokenized_text'] = test_data.apply(lambda x: word_tokenize(x['original_text'].lower()),axis=1)\n",
    "test_data.drop(columns=['original_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tokenized_text']=test_data.apply(lambda x: process(x['tokenized_text']),axis=1)\n",
    "X_test=test_data['tokenized_text'] \n",
    "y_test=test_data['label'] # label \n",
    "\n",
    "# text to sequence\n",
    "X_test=tokenizer.texts_to_sequences(X_test)\n",
    "X_test=pad_sequences(X_test,maxlen=1500)\n",
    "\n",
    "# predict\n",
    "y_pred= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list= [round(x[0]) for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# calculate the accuracy score\n",
    "accuracy_score(y_pred_list,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-gabriel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-pottery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-straight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
